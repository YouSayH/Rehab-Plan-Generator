# リハビリテーション総合実施計画書生成システム要件定義書

## 1. プロジェクト概要

### 1.1 プロジェクトの目的

本プロジェクトは、電子カルテ（EHR）のデータと連携し、大規模言語モデル（LLM）およびデータサイエンス技術を用いて、リハビリテーション総合実施計画書の「たたき台」を自動生成するシステムを構築することを目的とする。

### 1.2 解決する課題

* **業務効率化:** 患者情報の転記や文章作成にかかる時間を削減する。
* **属人化の解消:** ベテラン療法士の「臨床推論（どの情報を重視し、どう予後を予測するか）」をシステム化し、経験の浅い療法士でも質の高い計画書を作成できるようにする。
* **プライバシー保護:** 外部システム（AI側）に患者の個人情報（PHI）を永続保存しないセキュアな構成を実現する。

### 1.3 ターゲットユーザー

1. **新人セラピスト（メイン）:** 質の高い計画書作成の支援、臨床推論の補助を受ける。
2. **ベテランセラピスト:** ドラフト作成の自動化による業務時間短縮。
3. **学生:** ダミーデータを用いた模擬作成を通じ、計画書作成のプロセスを学習する。

---

## 2. システムアーキテクチャ方針

### 2.1 全体構成：ハイブリッド・ステートレス構成
本システムは、電子カルテシステムとは疎結合（Loosely Coupled）で連携し、原則としてシステム内部に個人情報を持つデータベースを保持しない「ステートレス」な設計とする。コードベースは疎結合を意識し、将来的なモジュール交換（例：検索エンジンの変更）に耐えうる設計とする。

### 2.2 ネットワーク・デプロイ環境

* **病院本番環境:** オンプレミスサーバー（または閉域網）にてDockerコンテナを用いて構築。外部インターネット接続は、クラウドLLM利用時のみ（プロキシ経由）または完全ローカルSLM運用の選択式とする。
* **教育・学習環境:** クラウドサーバー上でのデプロイを想定（学生が自宅等からアクセス可能）。

### 2.3 技術スタック（Tech Stack）

選定理由は Q&A の回答に基づく。

| レイヤー | 技術選定 | 選定理由・要件 |
| --- | --- | --- |
| **Frontend** | **React + TypeScript** | NotebookLM風の複雑な2ペイン構成、グラフ描画、状態管理のため。 スプレッドシート機能に **Univer** を採用。（Chainlit等のチャットUIフレームワークでは柔軟性が不足するためReactを採用）。|
| **Backend** | **Python 3.11 + FastAPI** | 非同期処理、データサイエンス（LightGBM等）、SLM統合のため。ORMに **SQLAlchemy 2.0 (Async)** を採用。 |
| **Database** | **PostgreSQL + pgvector** | 中間DB用。アプリデータ(JSONB)とベクトル検索を単一コンテナで統合管理。 |
| **ML/DS** | **LightGBM + Optuna** | 高速な推論と、開発時の自動チューニングのため。 |
| **AI Engine** | **Ollama** (Local) / **Gemini** (Cloud) | ローカルLLMとクラウドAPIの切り替え構成。タスク別SLM（SQL生成、要約等）の専門家集団構成を想定。 |
| **RAG Workflow** | **LangGraph** | 検索・生成・検証のループ処理や状態管理（Stateful Workflow）を実現するため。 |
| **NLP Engine** | **GiNZA (spaCy)** + **SudachiPy** | CLEAR手法の中核。 辞書ベースの高速なエンティティ抽出を行う（LLM不使用）。 |
| **Container** | **Docker / Docker Compose** | 環境差異の吸収と配布の容易化。 |

---

## 3. プライバシーとデータセキュリティ設計

本システムの最重要要件である「システム側にPHI（個人健康情報）を残さない」を実現するための設計。

### 3.1 分離表示アーキテクチャ（Frontend/Backend Separation）

「画面には名前を表示したいが、サーバー/AIには渡したくない」という要件を以下の仕組みで解決する。

1. **Backend (Python側):**
    * 電子カルテからデータを読み込んだ直後、メモリ上で氏名・住所・電話番号等の個人識別情報を、セッション毎に生成される `Hash_ID`（乱数ハッシュ）に置換する。
    * DB保存、ログ記録、AIへのプロンプト入力はすべて `Hash_ID` で行う。
    * **実名はメモリ上から即時破棄する。**


2. **Frontend (React側):**
    * ブラウザのメモリ上（Context/State）にのみ、一時的な対応表 `{ Hash_ID: "田中太郎" }` を保持する。
    * 画面描画時のみ、`Hash_ID` を実名に差し替えて表示する。
    * サーバー通信時（AI生成リクエスト等）は `Hash_ID` のみを送信する。



### 3.2 データのライフサイクル

* **一時データ:** ユーザーのセッション終了、またはブラウザタブを閉じた時点で、Frontendの対応表およびBackendの一時メモリは破棄される。
* **永続データ:** ベクトルDBやログには「匿名化済みデータ」のみが保存される。

### 3.3 外部出力（Excel等）
* **Excelプレビュー & 編集:** **Univer** を使用し、ブラウザ上でExcelライクな操作感を提供する。
* **クライアントサイド生成:** 最終的な `.xlsx` ファイル出力は、Univerのデータを元にクライアントサイド（ExcelJS/SheetJS等）で生成する。これにより、実名情報を含むファイルをサーバーを経由せずに作成可能とする。

====================== <br>
💡 エクスポート制限の回避策（アーキテクチャ）
Univerの有償機能である「Excel出力」に頼るのではなく、以下の方法をとります。これなら無償版の制限は関係ありません。

保存: Univer上のデータ（JSON形式）をそのままBackend (PostgreSQL) に保存する。

Excel出力: ユーザーが「Excelダウンロード」を押したら、以下のいずれかの方法で生成する。

Client側: exceljs 等のライブラリを使って、ブラウザ上でJSONからExcelを作る。

Server側: Pythonの openpyxl を使って、BackendでExcelを作って返す。

※今回はReact側で完結できる exceljs などの採用を検討するとスムーズです。

======================


---

## 4. 電子カルテ連携戦略

### 4.1 接続方式：中間DB（CSV連携）＋ オンデマンド更新

本番DBへの負荷とセキュリティリスクを最小限にするため、以下のハイブリッド方式を採用する。

1. **基本（バッチ連携）:**
    * 夜間バッチ等で電子カルテから出力されるCSV（または中間DB）を参照する。
    * 検索や分析は、この「昨日のデータ」に対して高速に行う。


2. **更新（オンデマンド）:**
    * 画面上に「最新情報を取得」ボタンを配置。
    * ユーザーが押下した時のみ、対象患者の最新情報をAPI（または直近データのクエリ）経由で取得し、メモリ上のデータを上書き更新する。



### 4.2 マスタデータ

* 薬剤名、疾患コード等は、接続先の病院で使用されている**院内マスタ**に準拠する。外部標準マスタへの無理な変換は行わない。

### 4.3 書き込み制限

* 本システムから電子カルテ側へのデータ書き込み（保存）は行わない（Read Only）。
* 生成結果はExcelファイルとしてローカルPCに保存し、必要であればユーザーが手動でカルテシステムへインポートする運用とする。



## 5. UI/UX デザイン要件

### 5.1 画面コンセプト：NotebookLM-like Interface

複雑な情報を整理しつつ、AIと対話しながら計画書を練り上げるため、**2ペイン（左右分割）構成**を採用する。

* **左ペイン：コンテキスト・エビデンス（Source & Context）**
* **患者基本情報カード:** 年齢、診断名、障害名、現病歴などの要約表示。
* **時系列グラフ:** 入院時から現在までのFIMスコア、バイタルデータ等の推移を可視化（Recharts等を使用）。
* **参照ソースリスト:** 
        * RAGによって検索された「類似症例の計画書」
        * 「関連するガイドライン」のカードリスト。
        * ユーザーが追加した「外部参照資料（Knowledge Base）」
        * クリックで詳細展開。
* **編集・シミュレーション:** ユーザーが左ペインの数値を変更（例：FIM運動項目を修正）すると、右ペインの生成内容に反映される(シュミレーション)機能を備える。


* **右ペイン：生成・ドラフト・チャット（Draft & Chat）**
* **計画書エディタ:** AIが生成した様式23の形式のプレビューが表示されるリッチテキストエディタ。ユーザーが直接加筆修正可能。
* **AIチャットボット:** 生成結果に対して「もう少し具体的に」「退院後の生活を見据えて」といった追加指示を出すための対話インターフェース。
* **Fact Checkハイライト:** 整合性チェック機能により矛盾が疑われる箇所を、赤線やツールチップで警告表示する。



### 5.2 インタラクション要件（Source Grounding）

* **引用元の可視化:**
* 生成された文章の文末に `[Source 1]` のような注釈リンクを付与する。
* ユーザーがリンクをクリック、またはカーソルを合わせると、左ペインの該当する参照資料がハイライトまたはポップアップ表示される。


* **Excelプレビュー & ダウンロード:**
* 作成完了後、ブラウザ上でExcel（様式23）のプレビューを表示し、クライアントサイドで `.xlsx` ファイルとしてダウンロード可能とする。



---

## 6. RAG（検索拡張生成）ロジック要件

### 6.1 検索戦略：Hybrid CLEAR (Clinical Entity Augmented Retrieval)

従来の「ベクトル検索（意味的類似）」の弱点である「医学的な厳密性の欠如」を克服するため、**LangGraph** 上で自律的な検索ワークフローを構築する。
**GiNZA (spaCy)** による高速なエンティティ抽出（定性）と、従来の**数値重み付け**（定量）を組み合わせ、計算リソースを抑えつつ高精度な検索を実現する。

#### Phase A: インジェスト（構造化・保存）
テキストデータを取り込む際、単なるベクトル化だけでなく、医学的な意味（エンティティ）を抽出して構造化データとして保存する。LLMは使用せず、軽量なNLPライブラリを使用することで高速処理を担保する。

* **NLPエンジン:** GiNZA (spaCy) + SudachiPy + 否定判定ロジック (`negation.py`)
* **辞書戦略（3層構造）:**
    * **L1 (Core):** SudachiDict（一般語彙）
    * **L2 (Medical):** 万病辞書, Hyakuyaku, ComeJis, DMiME（標準医療用語）
    * **L3 (Custom):** **ICFカテゴリ**（生活機能）, **標準身体部位**, **ユーザー辞書**（FIM評価語彙, リハビリ特有表現）
* **処理フロー:**
    1. 自由記述テキストから重要語句（病名・症状・ADL状態）を抽出。
    2. 係り受け解析により「否定（なし・認めず）」された語句を除外または否定フラグ付与。
    3. 抽出結果を `entities` タグとして **JSONB** カラムに、埋め込み表現を **pgvector** カラムに保存する。

#### Phase B: リトリーバル（検索実行）
**LangGraph** により、「クエリ分析 → フィルタリング → ランキング」のパイプラインを実行する。

1. **Step 1: Query Analysis (LLM)**
    * 対象患者の現在の状態（構造化データ＋直近の記録）をLLMに入力し、「検索すべきエンティティ条件（タグ）」と「重視すべき数値パラメータ」を生成する。
    * *例:* `Target: ["脳梗塞", "右片麻痺", "失語症"]`, `Weight: {"FIM_Motor": High}`

2. **Step 2: Hard Filter (SQL/JSONB)**
    * 生成されたタグを用い、PostgreSQLのJSONBインデックス（GIN Index）を活用して候補を物理的に絞り込む。
    * これにより、「大腿骨骨折」のような医学的に無関係な症例がベクトル検索に混入することを **100%防止** する。

3. **Step 3: Weighted Ranking (複合スコアリング)**
    * 絞り込まれた候補に対し、以下の要素で再ランク付けを行い、Top-K件を選定する。
    * **スコアモデル:**
      ```python
      total_score = (
          W1 * vector_similarity(文脈の一致度: pgvector) +
          W2 * entity_match_count(タグの一致数: CLEAR) - 
          W3 * numeric_distance(FIM点数や年齢の乖離)
      )
      ```
    * *補足:* `numeric_distance` は `1 - abs(target - candidate) / max_range` 等で正規化して扱う。

<!-- ### 6.1 検索戦略：ハイブリッド・リトリーバル

膨大な過去カルテから「真に参考になる症例」を見つけ出すため、2段階のフィルタリングと重み付けスコアリングを行う。

1. **Stage 1: 構造化フィルタ（SQL/Hard Filter）**
* 中間DB（PostgreSQL）に対し、SQLクエリで候補を絞り込む。
* *フィルタ条件例:* `疾患カテゴリが一致` AND `年齢 ±10歳` AND `入院時FIM合計 ±10点`。
* *目標:* 数万件 → 100件程度に絞る。


2. **Stage 2: ベクトル検索 & 複合スコアリング（Vector Search & Rerank）**
* **PostgreSQL (pgvector)** を使用。
* 絞り込まれた候補のテキストベクトルと、対象患者のテキストベクトルのコサイン類似度を計算。
    * また、あくまで例ではあるが、以下の計算式のように基づき類似度 `total_score` を算出し、上位 `K` 件を選定する。
    * **スコアモデル:**
      ```python
      total_score = (
          0.35 * disease_sim(ルールベース補正 + Embedding) +
          0.30 * function_sim(FIM等の数値距離) +
          0.15 * wish_sim(自由記述Embedding) +
          0.10 * social_sim(カテゴリ距離) +
          0.10 * plan_policy_sim(計画方針Embedding)
      )
      ```
    * **疾患類似度:** 単なるEmbeddingだけでなく、ルール（脳卒中 vs THAなら減点など）を併用する。
    * **機能評価:** FIM等は数値的な距離（`1 - abs(diff)/max`）を重視する。 -->

### 6.2 コンテキスト最適化（Context Optimization）

LLMのコンテキストウィンドウを節約し、生成精度を高めるための工夫。

* **エンティティ・ハイライト:** 検索された類似症例の全文を渡すのではなく、Phase Aで抽出された「重要エンティティ」周辺の文脈を優先的に抽出（Contextual Compression）してプロンプトに含める。
* **階層的要約:** 長期の入院記録については、日次記録ではなく「退院サマリ」や「週間要約」を優先して検索対象とする。

<!-- ### 6.2 長文コンテキスト対策（Hierarchical RAG）
電子カルテの膨大なテキストノイズを避けるため、以下の処理を行う。
* **チャンク化:** ノート単位（日付/セクション）で分割。
* **要約 (Summary SLM):** 上位チャンクを抽出後、専用のSLMで要旨（200~400トークン）を作成してから最終プロンプトに含める。
* **構造化抽出 (Extraction SLM):** FIM値、主診断、キーパーソン等は、事前に構造化データとして抽出しておく。 -->

### 6.3 外部知識ベース（Knowledge Base）
* ユーザーがPDFやテキスト（論文、マニュアル等）をアップロードできる機能。
* これらは匿名化・ベクトル化され、類似症例と同様にRAGの参照ソースとして利用される(病院ごとの方針・特徴が強く反映されると考えられるため、電子カルテとは別枠として検索してもいいかもしれません。)。

### 6.4 プロンプト構築（Context Injection）

LLMに渡すコンテキストは以下の構成とする。

* **System Prompt:** あなたはベテランの理学療法士です。以下の制約に従い、専門用語を適切に使用して計画書を作成してください。
* **Patient Context:** 対象患者の匿名化済み構造化データ（FIM推移、年齢、診断など）。
* **Reference Context (Few-Shot):** Stage 2で選ばれた類似症例の「当時の状態」と「実際に作成された計画書」のペア。
* **Prediction Context:** データサイエンスモデルが弾き出した「予後予測値」（後述）。

### 6.5 Fact Check（整合性確認）機能

* **トリガー:** ユーザーが任意のタイミングで押下する「整合性チェック」ボタンにより発動。
* **ロジック:** 軽量SLM（またはNLIモデル）を使用し、生成文の各文（Claim）が、左ペインの情報（Evidence）によって支持されるか（Entailment）、矛盾するか（Contradiction）を判定する。
* **実装:** 軽量なNLI（Natural Language Inference）モデル、または専用プロンプトを用いたLLMにより判定し、矛盾箇所をUI上で強調表示する。
---

## 7. データサイエンス・時系列予測要件

### 7.1 予測モデルの役割

LLMの「もっともらしい文章作成」を補完するため、統計的に確からしい数値予測を提供する。

* **目的変数:**
1. **退院時FIMスコア:** 各項目ごとの予測値。
2. **推定在院日数:** リハビリゴールまでの所要期間。
3. **転帰先確率:** 自宅復帰 / 施設入所 / 転院 の確率分布。



### 7.2 アルゴリズム選定

* **モデル:** **LightGBM** (勾配ブースティング決定木) を採用。
* *理由:* テーブルデータ（構造化データ）において高い精度が出やすく、欠損値に強く、学習・推論が高速であるため。また、データサイエンスのデファクトスタンダードである勾配ブースティング決定木である。


* **特徴量 (Features):**
* 静的因子: 年齢、性別、診断コード、発症からの期間。
* 動的因子: 入院時FIM、直近1週間のFIM変化率（回復速度）、合併症フラグ。

* **学習 (Training):** 夜間バッチ等で定期的に全過去データを用いて学習し、モデルファイルを更新する（Optunaによるチューニングは開発時または定期メンテナンス時に実施）。
* **推論 (Inference):** 画面表示時に学習済みモデルをロードし、リアルタイム（0.01秒以下）で予測を行う。


### 7.3 出力と連携

* 予測結果（例：「退院時予測FIM: 110点、確率80%」）は、グラフとして左ペインに表示すると同時に、LLMへのプロンプトにもテキストとして挿入し、文章生成の根拠とさせる。
* *生成文例:* 「統計的予測では〇〇点までの回復が見込まれるため、歩行自立を目標設定とします。」


## 8. データモデル設計（Schema Design）

**PostgreSQL (pgvector)** を採用し、詳細データは **JSONB** 型で管理する。

本システムはステートレス（状態を持たない）を基本思想としますが、分析の高速化とログ管理のために2種類のデータベースを保持します。

### 8.1 中間DB（Data Warehouse / Read-Only）

電子カルテから夜間バッチ等で抽出・同期される、分析専用のデータベースです。
**原則として個人を特定できる情報（氏名、住所等）は含まず、ハッシュ化されたIDで管理します。**

* **`patients_view` (患者属性)**
* `hash_id` (PK): 不可逆ハッシュ化された患者ID
* `age`: 年齢（または年代）
* `gender`: 性別
* `diagnosis_code`: ICD-10等の疾患コード
* `admission_date`: 入院日


* **`plan_data_store` (計画書詳細データ)**
* 300項目を超える様式データはカラム定義せず、JSONB型で柔軟に管理する。
* `hash_id` (FK): 患者ID
    * `doc_date`
    * `format_version`
* **`raw_data` (JSONB):** 様式23の全項目（ADL詳細、環境因子、プログラム等）を格納。

* **`documents_view` (テキスト記録)**
* `doc_id` (PK)
* `hash_id` (FK): 患者ID
* `doc_date`: 記録日
* `doc_type`: 書類タイプ（リハビリ実施録、看護サマリ等）
* `entities` (JSONB): GiNZA等で抽出された検索用タグ（例: `{"diagnosis": ["脳梗塞"], "symptoms": ["右片麻痺"]}`）。**GIN Index** を付与して高速検索を実現。
* `content_vector`: 本文のベクトル埋め込み（検索用）
* `summary_text`: 匿名化済みの要約テキスト
* `source_type`: 'EHR', 'UPLOAD'



### 8.2 アプリケーションDB（App DB / Logs）

システムの動作ログや、ユーザーからのフィードバックを保存します。

* **`audit_logs` (監査ログ)**
* `log_id` (PK)
* `timestamp`: 日時
* `user_id`: 操作したユーザー（職員ID）
* `target_hash_id`: 閲覧/生成対象の患者ハッシュID
* `action`: 操作内容（VIEW, GENERATE, EXPORT）


* **`feedback_data` (AI改善用)**
* `generation_id`: 生成ID
* `user_rating`: 評価（Good/Bad）
* `edit_distance`: AI生成文と、人間が最終的に修正した文の差異量（モデル精度向上の指標として利用）



---

## 9. 非機能要件（Non-Functional Requirements）

### 9.1 性能・レスポンス

* **検索速度:** 中間DBに対する類似症例検索は、数万件規模のデータに対し **3秒以内** に結果を返すこと（適切なインデックス設計）。
* **生成体験:** LLMの応答はストリーミング（逐次表示）を行い、**Time To First Token（最初の文字が出るまで）を 2秒以内** に抑えること。
* **クライアント負荷:** ブラウザ側でのExcel生成やグラフ描画が、一般的な業務用PC（メモリ8GB程度）で快適に動作すること。

### 9.2 可用性・耐障害性

* **フォールバック機能:** 外部クラウドLLM（Gemini等）への接続が切断された場合でも、ローカルSLM（Ollama）に自動で切り替わり、最低限の生成機能を継続できること（将来実装）。
* **エラー耐性:** 欠損値（例：FIMスコアの一部欠落）があってもシステムエラーで停止せず、「データ不足」を明示した上で処理を続行すること。

### 9.3 保守性・拡張性

* **Docker化:** アプリケーション（Frontend, Backend, DB）は `docker-compose` コマンド一つで環境構築・起動が可能であること。
* **EHRコネクタの分離:** 電子カルテごとの仕様差異（CSVの列名など）は `Connector` クラスで吸収し、コアロジックに影響を与えない設計とすること。

* **拡張性:** 新しい様式（JSON構造の変化）にDBスキーマ変更なしで対応できること（JSONB採用の理由）。
---

## 10. 運用フロー・ユースケース

### 10.1 標準業務フロー（新人セラピスト）

1. **ログイン:** 職員IDでログイン。
2. **患者選択:** 担当患者のIDを入力（またはリスト選択）。システムが中間DBからデータをロード。
3. **オンデマンド更新:** 必要に応じて「最新情報取得」ボタンを押し、直近のカルテデータを反映。
4. **プレビュー:** 左画面でFIM推移グラフや類似症例を確認。
5. **ドラフト生成:** AIが計画書案を作成。右画面でチャット形式で修正指示。
6. **Fact Check:** 「チェック」ボタンで矛盾点を確認・修正。
7. **出力:** Excel形式でダウンロードし、ブラウザを閉じる（データ破棄）。

### 10.2 学習フロー（学生）

1. **シナリオ選択:** 「脳梗塞・回復期」などのプリセットされたダミー患者セットを選択。
2. **シミュレーション:** 左画面のFIM点数をわざと低く変更し、計画書の内容（目標設定やアプローチ）がどう変化するかを確認する。
3. **根拠の学習:** AIが提示した参照リンク（ガイドライン等）を読み、臨床推論のプロセスを学ぶ。

---

## 11. 開発ロードマップ（Roadmap）

まずは **Phase 1** の完遂を目指します。

### **Phase 1: MVP構築** 【最優先】

* **目標:** React画面でダミーデータを表示し、簡単なAI生成ができる状態。
* **実装項目:**
* プロジェクトセットアップ（React + FastAPI + PostgreSQL + Docker）。
* 中間DB（PostgreSQL）の構築と、ダミーCSVデータのインポート機能。
* NotebookLM風の基本レイアウト（左右ペイン）の実装。
* Backendでの個人情報ハッシュ化ロジックの実装。
* PostgreSQL (pgvector) の構築と、JSONBを用いたスキーマ定義。
* ダミーCSVデータのインポート機能。
* Univer を組み込んだUIの実装。
* ハッシュ化によるプライバシー保護ロジックの実装。


### **Phase 2: 知能化（RAG & 検索）**

* **目標:** 類似患者検索が機能し、根拠に基づいた生成ができる状態。
* **実装項目:**
* 辞書ビルド: 万病辞書・ICF・ユーザー辞書等のコンパイルと `nlp_loader` への組み込み。
* インジェストパイプライン: GiNZAを用いたエンティティ抽出と `negation.py` 連携の実装（DBへのタグ保存）。
* 検索ワークフロー: **LangGraph** による `QueryAnalysis` -> `CLEAR Search` (SQL+Vector+スコアリング) -> `Generation` フローの実装。
* 外部資料（Knowledge Base）アップロード機能。
* 左ペインへの「参照ソース」「類似症例カード」の表示。
* 生成文からのソース引用（ハイライト）機能の実装。



### **Phase 3: 高度化（予測 & 運用）**

* **目標:** 時系列予測やFact Checkを備えた、実運用レベルの完成度。
* **実装項目:**
* 時系列予測モデル（LightGBM）の統合とグラフ表示。
* Fact Checkボタンと矛盾検知ロジックの実装。
* Excel出力（クライアントサイド生成）の実装。

---

## 11. 将来的な拡張構想 (Future Expansion)
* **プロンプト最適化:** OpenPrompt, DSPy, AdalFlow等の導入によるプロンプトエンジニアリングの自動化。
* **継続事前学習 (CPT):** 医療ドメイン知識を強化するためのLLMの追加学習。
* **マルチモーダル対応:** レントゲン画像やPDFカルテの解析・参照。
* **Agent化:** AutoGPT等を用いた、より自律的な調査・生成機能。
* **更新差分対応:** 電子カルテ更新時の差分検知と自動反映。
* **RPA連携:** APIが無い環境でのデータ取得手段の確保。

--

## 12. 旧リポジトリからの資産継承 (Legacy Asset Migration)

旧プロトタイプ (`kcr_Rehab-Plan-Generator`) のコードベースから、ドメイン知識が詰まった以下のモジュールを「コア資産」として新システムへ移植・適合させる。

### 12.1 必須資産 (Core Assets)
修正なし、またはデータ形式の変換のみで採用するモジュール。

| カテゴリ | 対象ファイル | 移行戦略・用途 |
| :--- | :--- | :--- |
| **データ定義** | `app/services/excel/mappings.py` | **【JSON化して採用】**<br>様式23の300以上のセル座標定義。JSONマスタに変換し、Frontend/Backend共通の座標定義として利用する。 |
| **出力雛形** | `template.xlsx` | **【そのまま採用】**<br>Excel出力のベーステンプレート。 |
| **NLPロジック** | `app/services/extraction/negation.py` | **【そのまま採用】**<br>GiNZAを用いた否定判定（「〜なし」「〜認めず」の検知）ロジック。**Phase 2: Hybrid CLEAR検索** の精度担保に不可欠。 |
| **辞書データ** | `user_dic.csv` | **【拡張利用】**<br>独自のリハビリ用語辞書。これをベースにICFコードやFIM用語を追加し、SudachiPy用のカスタム辞書(L3)を構築する。 |
| **プロンプト** | `app/services/llm/prompts.py` | **【テンプレート移植】**<br>「専門用語を避ける」「患者への配慮」など調整済みの指示文を、**LangChain PromptTemplate** に移植する。 |

### 12.2 参照・再利用資産 (Reference Assets)
ロジックやアルゴリズムを新アーキテクチャ（FastAPI/LangGraph）に合わせて書き換えて利用するモジュール。

| カテゴリ | 対象ファイル | 移行戦略・用途 |
| :--- | :--- | :--- |
| **バリデーション** | `app/models/plan.py` | **【ロジック再利用】**<br>各項目のデータ型（bool/str/int）定義を、新システムの **Pydanticモデル（スキーマ）** 作成時の設計図として使用する。 |
| **Ingest予備** | `app/services/llm/patient_info_parser.py` | **【予備実装】**<br>正規表現による非構造化テキスト解析ロジック。API/CSV連携ができない場合のバックアップ手段として `Ingest Pipeline` に組み込む。 |
| **Excel生成** | `app/services/excel/writer.py` | **【仕様参照】**<br>Python (`openpyxl`) コードは使用しないが、「性別を記号に変換する」等の**書き込み仕様**を、Frontend (`Univer/ExcelJS`) 実装時に参照する。 |
| **RAG設定** | `Rehab_RAG/rag_config.yaml` | **【設定参照】**<br>チャンクサイズや検索パラメータのベース値として利用する。 |